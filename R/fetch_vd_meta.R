#' Find vecdyn metadata
#'
#' @param ids a numeric ID or numeric vector of ids (preferably in an `ohvbd.ids` object) indicating the particular dataset/s to download.
#' @param rate maximum number of calls to the API per second.
#' @param connections number of simultaneous connections to the server at once. Maximum 8. **Do not enable unless you really need to** as this hits the server significantly harder than usual.
#' @param pb_name the name of the download progress bar
#' @param basereq an [httr2 request][httr2::request()] object, as generated by [vb_basereq()]. If `NA`, uses the default request.
#'
#' @returns list of `httr2::response` object
#'
#' @keywords internal
fetch_vd_meta <- function(
  ids,
  rate = 5,
  connections = 2,
  pb_name = "data",
  basereq = NA
) {
  max_conns <- 8

  if (!has_db(ids)) {
    cli::cli_alert_warning("IDs not necessarily from VecDyn.")
  } else if (!is_from(ids, "vd")) {
    cli::cli_abort(c(
      "x" = "IDs not from VecDyn, Please use the {.fn fetch_{ohvbd_db(ids)}} function.",
      "!" = "Detected database = {.val {ohvbd_db(ids)}}"
    ))
  }

  if (all(is.na(basereq))) {
    basereq <- vb_basereq()
  }

  reqs <- ids |>
    lapply(\(id) {
      basereq |>
        req_url_path_append("vecdyncsv") |>
        req_url_query("format" = "json", "piids" = id) |>
        req_error(body = vd_error_body) |>
        req_headers(ohvbd = id) |> # Add additional header just so we can nicely handle failures
        req_throttle(rate)
    })

  if (connections > max_conns) {
    cli::cli_alert_warning(
      "No more than {.val {max_conns}} simultaneous connection{?s} allowed!"
    )
    cli::cli_alert_info("Restricting to {.val {max_conns}} connection{?s}.")
    connections <- max_conns
  }
  resps <- reqs |>
    req_perform_parallel(
      on_error = "continue",
      max_active = connections,
      progress = list(
        name = pb_name,
        format = "Finding {cli::pb_name} {cli::pb_current}/{cli::pb_total} {cli::pb_bar} {cli::pb_percent} | ETA: {cli::pb_eta}"
      )
    )
  return(resps)
}

#' @title Fetch VecDyn dataset length by ID
#' @description Retrieve length of VecDyn dataset/s specified by their dataset ID.
#' @author Francis Windram
#'
#' @param ids a numeric ID or numeric vector of ids (preferably in an `ohvbd.ids` object) indicating the particular dataset/s to download.
#' @param rate maximum number of calls to the API per second.
#' @param connections number of simultaneous connections to the server at once. Maximum 8. **Do not enable unless you really need to** as this hits the server significantly harder than usual.
#' @param page_size the page size returned by VecDyn (default is 50).
#' @param basereq an [httr2 request][httr2::request()] object, as generated by [vb_basereq()]. If `NA`, uses the default request.
#'
#' @return A dataframe describing the number of rows and number of pages for the set of ids.
#'
#' @examplesIf interactive()
#' fetch_vd_counts(54)
#'
#' fetch_vd_counts(c(423,424,425))
#'
#' @concept vecdyn
#'
#' @export
#'

fetch_vd_counts <- function(
  ids,
  rate = 5,
  connections = 2,
  page_size = 50,
  basereq = NA
) {
  count_resps <- fetch_vd_meta(
    ids,
    rate,
    connections,
    basereq,
    pb_name = "VecDyn data counts"
  )

  # Find counts and calculate required pages
  resp_parsed <- count_resps |>
    lapply(\(resp) resp_body_json(resp)$count) |>
    as.numeric()
  resp_parsed <- data.frame(id = ids, num = resp_parsed)
  resp_parsed$pages <- ceiling(resp_parsed$num / page_size)
  return(resp_parsed)
}

#' @title Fetch VecDyn metadata table
#' @description Fetch VecDyn metadata table (downloading if necessary) and cache if fresh.
#' @author Francis Windram
#'
#' @param cache_location path to cache location (defaults to user directory obtained from [tools::R_user_dir()][tools::R_user_dir()]).
#' @param refresh_cache force a refresh of the relevant cached data.
#' @param noprogress disable non-essential messaging (progress bars etc.).
#' @param basereq an [httr2 request][httr2::request()] object, as generated by [vb_basereq()]. If `NA`, uses the default request.
#'
#' @return A dataframe describing the current VecDyn metadata.
#'
#' @examplesIf interactive()
#' fetch_vd_meta_table()
#'
#' @concept vecdyn
fetch_vd_meta_table <- function(cache_location = NULL, refresh_cache = FALSE, noprogress = FALSE, basereq = NA) {

  cache_loaded <- FALSE

  if (all(is.na(basereq))) {
    basereq <- vb_basereq()
  }

  cache_location <- cache_location %||% get_default_ohvbd_cache("vecdyn")
  file_location <- file.path(cache_location, "vd_meta_table.rds")

  # Check cache
  if (file.exists(file_location) && !refresh_cache) {
    meta_table <- readRDS(file_location)
    cached_writetime <- attr(meta_table, "writetime", exact = TRUE)
    # If timestamp is present and data is not stale then load and call it a day
    if (!is.null(cached_writetime) && lubridate::now() - cached_writetime < days(1)) {
      cache_loaded <- TRUE
      if (!noprogress) {
        cli::cli_alert_success("Loaded from cache.")
      }
    } else {
      cli::cli_alert_warning("Cached metadata table is stale and must be re-downloaded. Download timestamp: {cached_writetime}.")
    }
  }

  # If we didn't load (or decided not to use) cached data, download it fresh!
  if (!cache_loaded) {
    progress_format <- list(format = "{cli::pb_spin} Downloading VecDyn metadata table...")
    if (noprogress) {
      progress_format <- FALSE
    }

    # Download metadata table from vecdyn
    res <- basereq |>
      httr2::req_url_path_append("vecdynbyprovider") |>
      httr2::req_throttle(5) |>
      httr2::req_perform_iterative(
        httr2::iterate_with_offset("page", resp_complete = \(resp) {
          # Check to make sure there's not another page to find.
          httr2::resp_body_json(resp)$data["next"] == "NULL"
        }),
        progress = progress_format)

    # Extract and parse multi-element lists
    meta_table <- res |>
      resps_successes() |>
      resps_data(\(resp) resp_body_json(resp)$data$results)

    meta_table <- sapply(meta_table, \(x) {
      x["SpeciesName"] <- paste(unlist(x["SpeciesName"]), collapse = ", ")
      x["Years"] <- paste(unlist(x["Years"]), collapse = ", ")
      x["Tags"] <- paste(unlist(x["Tags"]), collapse = ", ")
      x["CollectionMethods"] <- paste(unlist(x["CollectionMethods"]), collapse = ", ")
      x["AverageGPSCoords"] <- paste(unlist(x["AverageGPSCoords"]), collapse = ", ")
      x
    })

    meta_table <- as.data.frame(t(meta_table))

    attr(meta_table, "writetime") <- lubridate::now()
  }

  # Save to cache if we performed a download
  if (!cache_loaded) {
    saveRDS(meta_table, file_location)
    if (!noprogress) {
      cli::cli_alert_success("Saved to cache.")
    }
  }

  return(meta_table)
}
