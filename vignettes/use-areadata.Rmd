---
title: "Sourcing climatic data from AREAdata"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Sourcing climatic data from AREAdata}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r earlypkgload, include=FALSE}
library(ohvbd)
runstart <- lubridate::now()
```

<!-- Shim to make sure vignette builds on ubuntu-->
```{r options, include=FALSE}
get_os <- function() {
  sysinf <- Sys.info()
  if (!is.null(sysinf)) {
    os <- sysinf["sysname"]
    if (os == "Darwin")
      os <- "osx"
  } else { ## mystery machine
    os <- .Platform$OS.type
    if (grepl("^darwin", R.version$os))
      os <- "osx"
    if (grepl("linux-gnu", R.version$os))
      os <- "linux"
  }
  tolower(os)
}
if (get_os() == "linux") {
  set_ohvbd_compat()
}

# Suppress cli output in non-interactive mode
if (!rlang::is_interactive()) {
  options(cli.default_handler = function(msg) invisible(NULL))
} else {
  options(cli.default_handler = NULL)
}

```


## Introduction

Often when presented with a set of population dynamic data, we may wish to investigate the abiotic conditions such as temperature or humidity that these dynamics occurred under.

Ideally this sort of data would have been collected at the same time as the dynamics data. However with data collected for one project and then repurposed for another, this is often not possible.

In such scenarios it is possible to link the location of the data with other measurements of abiotic variables in the area.

These can be retrieved from raster files provided by projects such as the ERA5 data store from the Copernicus project, however often this is a large amount of data for a small number of datapoints.

## AREAdata

An alternative method for gathering this data is through the AREAdata project, which provides historical and forecast data aggregated at different spatial scales.

Let's take a look at the format of data retrieved from AREAdata (we will deconstruct this command properly in a bit):

```{r example_ad}
library(ohvbd)

ad_df <- fetch_ad(metric = "temp", gid = 0, use_cache = TRUE)
cat("Data dimensions: ", ncol(ad_df), " cols x ", nrow(ad_df), " rows")
```

```{r ad_num, include=FALSE}
ad_num <- ncol(ad_df) * nrow(ad_df)
```


So that is quite a lot of data! Way too much to print here, given that it's `r ad_num` entries.

AREAdata outputs are formatted as matrices. Here the rows correspond to locations (identified by GADM codes):

```{r row_names}
head(rownames(ad_df))
```

Meanwhile the column names correspond to the day that the data refers to:

```{r col_names}
head(colnames(ad_df))
```

So the value at `ad_df["Angola", "2020-01-01"]` corresponds to the value of the metric (in this case temperature) in Angola on 2020-01-01:

```{r individual_row}
ad_df["Angola", "2020-01-01"]
```
## Deconstructing `fetch_ad()`

That call above to `fetch_ad()` used some slightly odd parameters, so let's break that down a bit.

The arguments we provided were

1. metric = "temp"
2. gid = 0
3. use_cache = TRUE
4. cache_location = cachepath

### `metric`

The metric argument tells `fetch_ad()` what type of data it should retrieve from AREAdata.

There are many valid options for this including temperature and humidity. (See [AREAdata](https://pearselab.github.io/areadata/download-links/) or run `?fetch_ad()` in the R console for a full list).

### `gid`

The gid argument tells `fetch_ad()` the spatial scale to download (with higher numbers corresponding to finer scales, at the cost of larger download sizes).

### `use_cache` and `cache_location`

As the AREAdata files are large and monolithic, it is often advantageous to download data at an given spatial scale only once.

We can do this by caching the files and retrieving them as needed. Whilst you could do this manually, `ohvbd` already has the functionality to perform this behind the scenes.

If we set `use_cache = TRUE` then instead of downloading from the repository immediately, `ohvbd` will first try to load any cached version that may be present.

Typically, the cache location is a folder in your user directory, obtained from `tools::R_user_dir()`. However, if you want to overwrite it (such as if you want to share the cache between different scripts) you can provide an alternative path to save data to.

### Example

So given the above, let's construct another function call to download relative humidity data at the province level (gid 1), caching in the same folder.

(We will not execute this, just because gid level 1 data is significantly larger than gid level 0)

```{r relhumid, echo=TRUE, eval=FALSE}
fetch_ad(metric = "relhumid", gid = 1, use_cache = TRUE)
```

## Extracting specific data

Extracting data from this matrix can be fiddly, but luckily `ohvbd` also provides a function to aid in that: `extract_ad()`.

So let's again try to extract the same data as previously, the temperature in Angola on 2020-01-01:

```{r extract_basic}
ad_df %>% extract_ad(targetdate = "2020-01-01", places = "Angola")
```
*Note: ignore the `attr` part of the output, this data just allows `ohvbd` to ensure consistency between its functions.*


Wonderful! But what if we want a range of dates, or many places? What if the dates fall outside the dates that we have data for, or the places are not actually present in the data?

This is where `extract_ad()` really shines.

### Dates and date ranges

Let's first look at the flexibility of the date arguments to `extract_ad()`.

If we (for example) want not just one day, we can specify either a vector of dates, or a less exact date as `targetdate`

```{r extract_multidate}
ad_df %>% extract_ad(
  targetdate = c("2020-01-01", "2020-01-02", "2020-01-05"),
  places = "Angola"
)
```

```{r extract_month}
ad_df %>% extract_ad(targetdate = "2020-01", places = "Angola")
```

```{r extract_year}
# 366 days of data return as 2020 is a leap year
length(ad_df %>% extract_ad(targetdate = "2020", places = "Angola"))
```

If we would instead like to specify a date range, we can add the argument `enddate`.

_Note: the end date is defined **exclusively**, meaning that any date before it (and on or after the target date) is considered to be "in"._

```{r extract_range}
ad_df %>% extract_ad(
  targetdate = "2020-01-01",
  enddate = "2020-01-04",
  places = "Angola"
)
```

### Places

Multiple places to extract data for can also be provided as a vector of places:

```{r extract_places}
ad_df %>% extract_ad(
  targetdate = "2020-01-01",
  places = c("Angola", "Latvia", "United Kingdom")
)
```

This can be combined with the date filters above to create a more restrictive extraction:

```{r extract_exact}
ad_df %>% extract_ad(
  targetdate = "2020-01-01",
  enddate = "2020-01-04",
  places = c("Angola", "Latvia", "United Kingdom")
)
```

### Resiliance

The `extract_ad()` function is also relatively resilient to the use of dates or places that are not in AREAdata:

```{r extract_missing}
ad_df %>% extract_ad(
  targetdate = "2023-12-30",
  enddate = "2024-01-02",
  places = c("Atlantis", "Latvia", "United Kingdom")
)
```

## To be completed
```{r tot_time, include=FALSE}
tot_time <- lubridate::as.duration(lubridate::now() - runstart)
```

<span style="opacity: 0.1;font-size: small;">Built in `r tot_time`s</span>
